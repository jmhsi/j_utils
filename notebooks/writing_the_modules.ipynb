{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# munging.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T21:15:31.032664Z",
     "start_time": "2019-10-17T21:15:31.018276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id  2\n",
      "0  a  x\n",
      "1  b  y\n",
      "2  c  z   id sort\n",
      "0  a    j\n",
      "1  b    l\n",
      "2  c    k\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.DataFrame({'id': ['a', 'b', 'c'], 2: ['x', 'y', 'z']})\n",
    "b = pd.DataFrame({'id': ['a', 'b', 'c'], 'sort': ['j', 'l', 'k']})\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T19:49:52.101940Z",
     "start_time": "2019-10-17T19:49:52.094204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0, 'c': 1, 'b': 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort = b.sort_values('sort')['id']\n",
    "sort_map = dict(zip(sort, range(len(sort))))\n",
    "sort_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T21:15:13.214712Z",
     "start_time": "2019-10-17T21:15:13.200602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  2\n",
       "0  a  x\n",
       "2  c  z\n",
       "1  b  y"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['sort_temp'] = a['id'].map(sort_map)\n",
    "a = a.sort_values('sort_temp')\n",
    "a = a.drop('sort_temp', axis=1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T22:52:12.322719Z",
     "start_time": "2019-10-17T22:52:12.308979Z"
    },
    "code_folding": [
     66,
     108,
     119,
     127,
     147,
     154,
     163,
     194,
     198,
     208,
     213,
     244,
     253,
     274,
     282
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../j_utils/munging.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../j_utils/munging.py\n",
    "# %load munging.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Training Utils?\n",
    "def time_series_data_split(eva, date_col, q, n_yields):\n",
    "    '''\n",
    "    passing in sorted eva df and date_col, returns train and valid,\n",
    "    val_frac as valid data size. uses panda qcut behind the scenes\n",
    "    returns n_yields number of sets of indicies\n",
    "    '''\n",
    "#     step = int(val_frac * len(eva))\n",
    "    # get date in date_col at split, return everything\n",
    "    eva = eva.copy()\n",
    "    eva['temp'] = pd.qcut(eva['issue_d'], q=q, labels=range(q))\n",
    "    counter = 0\n",
    "    for i in range(q-1, q-1-n_yields, -1):\n",
    "        val_idx = eva.query('temp == @i').index\n",
    "        counter -= len(val_idx)\n",
    "        tr_idx = eva.index[:counter]\n",
    "        yield tr_idx, val_idx\n",
    "\n",
    "\n",
    "def sort_train_eval(train, eva, id_col, sort_col, assert_shape=True):\n",
    "    '''\n",
    "    Helps to sort train df by sort_col which is only in eva df. train and\n",
    "    eva should both contain id_col (the unique identifier).\n",
    "    assert_shape checks that every id_col in train and eva are one-to-one\n",
    "    '''\n",
    "    if assert_shape:\n",
    "        train_id = set(train[id_col])\n",
    "        eva_id = set(eva[id_col])\n",
    "        assert not train.duplicated(id_col).any(), print('first df has id dupes')\n",
    "        assert not eva.duplicated(id_col).any(), print('second df has id dupes')\n",
    "        assert train_id == eva_id, print('some ids not present in both dfs')\n",
    "        assert train.shape[0] == eva.shape[0], print('ids not one to one')\n",
    "    eva = eva.sort_values(sort_col)\n",
    "    sorted_id = eva[id_col]\n",
    "    sort_map = dict(zip(sorted_id, range(len(sorted_id))))\n",
    "    train['sort_temp'] = train[id_col].map(sort_map)\n",
    "    train = train.sort_values('sort_temp')\n",
    "    train = train.drop('sort_temp', axis=1)\n",
    "    return train, eva\n",
    "    \n",
    "\n",
    "def check_train_valid(train, valid, id_col, original=None):\n",
    "    '''\n",
    "    Function to check that two df's (train, valid) do not contain the same\n",
    "    data samples based on id_col. If passing original full set, it checks\n",
    "    that all samples are accounted for as well (if splitting train and valid\n",
    "    from original)\n",
    "    '''\n",
    "    train_id = set(train[id_col])\n",
    "    test_id = set(valid[id_col])\n",
    "    assert len(train_id.intersection(test_id)) == 0\n",
    "    if original:\n",
    "        assert train_id.union(test_id) == set(original[id_col])\n",
    "        print('all ids accounted for')\n",
    "    print('no overlapping id cols found')\n",
    "\n",
    "# MUNGING ____________________________________________________________________\n",
    "def train_proc(df, normalize = True, verbose=True, isnull=True):\n",
    "    '''\n",
    "    get_noninf_val, replace_infs with respective max or min value\n",
    "    get_normalize_info, normalize_df (if needed)\n",
    "    transform dates\n",
    "    make_null_ind_cols\n",
    "    get_fill_values, fill_values\n",
    "    get_categories, encode_categories\n",
    "    remove_zerovar_cols\n",
    "    \n",
    "    Returns list of:\n",
    "    df, all_train_colnames, max_dict, min_dict, new_null_colnames, fill_dict, cats_dict, (norm_dict)\n",
    "    '''\n",
    "\n",
    "    max_dict, min_dict = get_noninf_val(df)\n",
    "    replace_infs(df, max_dict, min_dict)\n",
    "    if normalize:\n",
    "        norm_dict = get_normalize_info(df)\n",
    "        normalize_df(df, norm_dict)\n",
    "    transform_dates(df)\n",
    "    if isnull:\n",
    "        df, new_null_colnames = make_null_ind_cols(df)\n",
    "    fill_dict = get_fill_values(df)\n",
    "    fill_values(df, fill_dict)\n",
    "    cats_dict = get_categories(df)\n",
    "    encode_categories(df, cats_dict)\n",
    "    remove_zerovar_cols(df, verbose=verbose)\n",
    "    to_ret = [df, df.columns, max_dict, min_dict, fill_dict, cats_dict]\n",
    "    if isnull:\n",
    "        to_ret.append(new_null_colnames)\n",
    "    if normalize:\n",
    "        to_ret.append(norm_dict)\n",
    "    return to_ret\n",
    "\n",
    "def val_test_proc(df, all_train_colnames, max_dict, min_dict, fill_dict, cats_dict, norm_dict={}, verbose=True, isnull = True):\n",
    "    '''\n",
    "    # Make cols match\n",
    "    # replace_infs\n",
    "    # noramlize_df (if needed)\n",
    "    # transform dates\n",
    "    # make_null_ind_cols\n",
    "    # fill_values\n",
    "    # encode_categories\n",
    "    # select cols to match train\n",
    "    '''\n",
    "    for col in all_train_colnames:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    \n",
    "    transform_dates(df)\n",
    "    replace_infs(df, max_dict, min_dict)\n",
    "    if norm_dict:\n",
    "        normalize_df(df, norm_dict)\n",
    "    if isnull:\n",
    "        df, new_null_colnames = make_null_ind_cols(df)\n",
    "    fill_values(df, fill_dict)\n",
    "    encode_categories(df, cats_dict)\n",
    "    df = df[all_train_colnames]\n",
    "\n",
    "    return df\n",
    "# sub_functions\n",
    "\n",
    "def add_datepart(df, fldname, drop=True, time=False):\n",
    "    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n",
    "    the information from the date. This applies changes inplace.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: A pandas data frame. df gain several new columns.\n",
    "    fldname: A string that is the name of the date column you wish to expand.\n",
    "        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n",
    "    drop: If true then the original date column will be removed.\n",
    "    time: If true time features: Hour, Minute, Second will be added.\n",
    "\n",
    "    Examples:\n",
    "    ---------\n",
    "\n",
    "    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000'], infer_datetime_format=False) })\n",
    "    >>> df\n",
    "\n",
    "        A\n",
    "    0   2000-03-11\n",
    "    1   2000-03-12\n",
    "    2   2000-03-13\n",
    "\n",
    "    >>> add_datepart(df, 'A')\n",
    "    >>> df\n",
    "\n",
    "        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed\n",
    "    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800\n",
    "    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200\n",
    "    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600\n",
    "    \"\"\"\n",
    "    fld = df[fldname]\n",
    "    if not np.issubdtype(fld.dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second', 'Microsecond', 'Nanosecond']\n",
    "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "#     df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "\n",
    "def transform_dates(df):\n",
    "    '''\n",
    "    Looks for datetime columns in the df, uses fastai's add_datepart to turn it\n",
    "    into several columns (year, day, is quarter end, etc.)\n",
    "    Does this inplace.\n",
    "    '''\n",
    "    date_cols = list(df.select_dtypes('datetime').columns)\n",
    "    for col in date_cols:\n",
    "        add_datepart(df, col,)\n",
    "\n",
    "\n",
    "def get_inf_cols(df):\n",
    "    '''\n",
    "    Gets colnames with inf in them\n",
    "    '''\n",
    "    num_cols = df.select_dtypes([np.number]).columns\n",
    "    inf_cols = [col for col in num_cols if any(np.isinf(df[col]))]\n",
    "    return inf_cols\n",
    "\n",
    "def get_noninf_val(df):\n",
    "    inf_cols = get_inf_cols(df)\n",
    "    max_dict = df[inf_cols].replace([np.inf], np.nan).max().to_dict()\n",
    "    min_dict = df[inf_cols].replace([-np.inf], np.nan).min().to_dict()\n",
    "    for k,v in max_dict.items():\n",
    "        max_dict[k] = v*100 #was inf so make it 100x larger than max\n",
    "    for k,v in min_dict.items():\n",
    "        if v > 0:\n",
    "            v = v/100\n",
    "        elif v <= 0:\n",
    "            v = v*100\n",
    "        min_dict[k] = v #was -inf so make it 100x larger than min    \n",
    "    return max_dict, min_dict\n",
    "\n",
    "def replace_infs(df, max_dict, min_dict):\n",
    "    for col, val in max_dict.items():\n",
    "        df[col] = df[col].replace([np.inf], val)\n",
    "    for col, val in min_dict.items():\n",
    "        df[col] = df[col].replace([-np.inf], val)\n",
    "                \n",
    "def get_null_cols(df):\n",
    "    '''\n",
    "    Gets null colnames in df if there are any\n",
    "    '''\n",
    "    null_cols = [col for col in df.columns if any(df[col].isnull())]\n",
    "    return null_cols\n",
    "\n",
    "def make_null_ind_cols(df):\n",
    "    '''\n",
    "    Had to return or else cols weren't actually added inplace\n",
    "    '''\n",
    "    null_cols = get_null_cols(df)\n",
    "    for col in null_cols:\n",
    "        df[col+'_isnull'] = np.where(df[col].isnull(), 1, 0)\n",
    "    return df, [col+'_isnull' for col in null_cols]        \n",
    "        \n",
    "def get_fill_values(df):\n",
    "    '''\n",
    "    Only to be used on train. Get the right fill values for every col.\n",
    "    '''\n",
    "    assert len(df.columns) == len(df.select_dtypes([np.number, 'object', 'bool']).columns)\n",
    "    all_cols = df.columns\n",
    "    fill_dict = {}\n",
    "    for col in all_cols:\n",
    "        if is_numeric_dtype(df[col]):\n",
    "            fill_val = df[col].median() # take median\n",
    "            if not np.isfinite(fill_val):\n",
    "                try:\n",
    "                    fill_val = df[col].mode()[0] # take mode\n",
    "                except IndexError:\n",
    "                    fill_val = 0\n",
    "                finally:\n",
    "                    if not np.isfinite(fill_val):\n",
    "                        fill_val = 0\n",
    "                \n",
    "        elif is_string_dtype(df[col]):\n",
    "            try:\n",
    "                fill_val = df[col].mode()[0] # take mode\n",
    "            except IndexError:\n",
    "                fill_val = '0'\n",
    "            \n",
    "        # if fill val still not finite, set to 0\n",
    "#         if not np.isfinite(fill_val):\n",
    "#             fill_val = 0 # if mode is nan or inf\n",
    "        fill_dict[col] = fill_val\n",
    "    return fill_dict\n",
    "\n",
    "def fill_values(df, fill_dict):\n",
    "#     assert set(df.columns) == set(fill_dict.keys())\n",
    "    df.fillna(fill_dict, inplace=True)\n",
    "\n",
    "def get_categories(df):\n",
    "    '''\n",
    "    only gets categories.\n",
    "    '''\n",
    "    obj_cols = df.select_dtypes('object')\n",
    "    cats_dict = {}\n",
    "    for col in obj_cols:\n",
    "        cats_dict[col] = df[col].astype('category').cat.categories\n",
    "    return cats_dict\n",
    "            \n",
    "def encode_categories(df, cats_dict):\n",
    "    for col, cats in cats_dict.items():\n",
    "        df[col] = pd.Categorical(df[col], categories = cats)\n",
    "        df[col] = df[col].cat.codes + 1 #so nan -> -1 becomes 0\n",
    "            \n",
    "def remove_zerovar_cols(df, ret_cols=False, verbose=False):\n",
    "    '''\n",
    "    Iterates throught columns and checks nunique(). If nunique == 1, then the whole\n",
    "    column has only one value, and will be dropped inplace. Prints out which\n",
    "    columns it has dropped and can return them if desired.\n",
    "    '''\n",
    "    zerovar_cols = []\n",
    "    consider_drop_cols = []\n",
    "    for col in df.columns:\n",
    "        nunique = df[col].nunique(dropna=False)\n",
    "        if nunique <= 1:\n",
    "            zerovar_cols.append(col)\n",
    "        else:\n",
    "            var = df[col].var()\n",
    "            if (not np.isfinite(var)) | (var == 0):\n",
    "                zerovar_cols.append(col)\n",
    "    df.drop(zerovar_cols, axis=1, inplace=True)\n",
    "\n",
    "            \n",
    "    for col in df.columns:\n",
    "        nunique = df[col].nunique(dropna=False)\n",
    "        if nunique == 2:\n",
    "            if '_isnull' not in col:\n",
    "                consider_drop_cols.append(col)\n",
    "\n",
    "    if verbose:\n",
    "        print('dropping the following cols: \\n{0}'.format(zerovar_cols))           \n",
    "        print('only 2 values, consider dropping the following cols: \\n{0}'.format(consider_drop_cols))\n",
    "    if ret_cols:\n",
    "        return zerovar_cols, consider_drop_cols        \n",
    "        \n",
    "def all_numeric(df):\n",
    "    '''\n",
    "    Returns True if the df is completely numeric.\n",
    "    '''\n",
    "    if df.shape[1] == df._get_numeric_data().shape[1]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_normalize_info(df):\n",
    "    num_df = df.select_dtypes(np.number)\n",
    "    assert all_numeric(num_df)\n",
    "    norm_dict = {}\n",
    "    means = {}\n",
    "    stds = {}\n",
    "    rel_cols = [col for col in num_df.columns if '_isnull' not in col]\n",
    "    std_notfin = {}\n",
    "    for col in rel_cols:\n",
    "        mean = num_df[col].mean()\n",
    "        std = num_df[col].std()\n",
    "        if np.isfinite(std):\n",
    "            means[col] = mean\n",
    "            stds[col] = std\n",
    "        else:\n",
    "            std_notfin[col] = std\n",
    "    norm_dict['means'] = means\n",
    "    norm_dict['stds'] = stds\n",
    "    norm_dict['std_notfin'] = std_notfin\n",
    "    return norm_dict\n",
    "\n",
    "def normalize_df(df, norm_dict):\n",
    "    means = norm_dict['means']\n",
    "    stds = norm_dict['stds']\n",
    "    for col in means.keys():\n",
    "        mean = means[col]\n",
    "        std = stds[col]\n",
    "        df[col] = (df[col]-mean)/std\n",
    "        \n",
    "def reduce_memory(df, verbose=True):\n",
    "    '''\n",
    "    Tries to cast float and int dtype cols to smallest possible for dataframe.\n",
    "    For saving RAM/disk space.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('trying to change columns to smaller dtypes when possible')\n",
    "        ori_mem = df.memory_usage(deep=True).sum()\n",
    "        print('original dataframe is {0} MB or {1} GB'.format(ori_mem/(1024**2), ori_mem/(1024**3)))\n",
    "    dict_to_df = {}\n",
    "    changed_type_cols = []\n",
    "    reducible = df.select_dtypes(['int', 'float'])\n",
    "    irreducible = df[[col for col in df.columns if col not in reducible.columns]]\n",
    "    for col in tqdm(reducible.columns):\n",
    "        col_type = df[col].dtypes.name\n",
    "        max_val = df[col].max()\n",
    "        min_val = df[col].min()\n",
    "        int_types = ['int32', 'int16', 'int8']\n",
    "        float_types = ['float32'] #float 16 not supported in feather format?, 'float16']\n",
    "        np_typedict = np.typeDict\n",
    "        if 'float' in col_type:\n",
    "            type_list = float_types\n",
    "            infoer = np.finfo\n",
    "        elif 'int' in col_type:\n",
    "            type_list = int_types\n",
    "            infoer = np.iinfo\n",
    "        ok_dtypes = []\n",
    "        for dtype in type_list:\n",
    "            dt_max = infoer(dtype).max\n",
    "            dt_min = infoer(dtype).min\n",
    "            if (max_val <= dt_max) & (min_val >= dt_min):\n",
    "                ok_dtypes.append(dtype)\n",
    "        try:\n",
    "            cast_dtype = ok_dtypes[-1]\n",
    "        except IndexError:\n",
    "            cast_dtype = col_type\n",
    "        if cast_dtype != col_type:\n",
    "            dict_to_df[col] = df[col].astype(cast_dtype)\n",
    "            changed_type_cols.append(col)\n",
    "    print('changed dtypes of {0} cols'.format(len(changed_type_cols)))\n",
    "    reduced = pd.DataFrame(dict_to_df)\n",
    "    smaller = pd.concat([irreducible, reduced], axis=1)        \n",
    "    if verbose:\n",
    "        small_mem = smaller.memory_usage(deep=True).sum()\n",
    "        print('reduced dataframe is {0} MB or {1} GB'.format(small_mem/(1024**2), small_mem/(1024**3)))\n",
    "    return changed_type_cols, smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# file_manipulation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T03:33:56.555072Z",
     "start_time": "2019-06-17T03:33:56.535815Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "from pwd import getpwnam, getpwuid\n",
    "import shutil\n",
    "\n",
    "\n",
    "def find_owner(filename):\n",
    "    return getpwuid(os.stat(filename).st_uid).pw_name\n",
    "\n",
    "def is_group_writeable(filepath):\n",
    "    '''\n",
    "    https://stackoverflow.com/questions/1861836/checking-file-permissions-in-linux-with-python\n",
    "    '''\n",
    "    st = os.stat(filepath)\n",
    "    return bool(st.st_mode & stat.S_IWGRP)\n",
    "\n",
    "def make_group_writeable(filepath):\n",
    "    if not is_group_writeable(filepath):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:38:50.054762Z",
     "start_time": "2019-06-17T02:38:50.031260Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jenkins'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpath = '/home/justin/projects/lendingclub/data/'\n",
    "find_owner(dpath + 'raw_loan_info.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T03:34:04.917444Z",
     "start_time": "2019-06-17T03:34:04.680983Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42mcsvs\u001b[0m/  example.py  raw_loan_info.fth  raw_pmt_hist_1.fth\r\n"
     ]
    }
   ],
   "source": [
    "ls {dpath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T03:34:06.112417Z",
     "start_time": "2019-06-17T03:34:06.091551Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pwd.struct_passwd(pw_name='justin', pw_passwd='x', pw_uid=1000, pw_gid=1000, pw_gecos='Justin Hsi,,,', pw_dir='/home/justin', pw_shell='/bin/bash')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getpwnam('justin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T03:34:06.411344Z",
     "start_time": "2019-06-17T03:34:06.390311Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pwd.struct_passwd(pw_name='jenkins', pw_passwd='x', pw_uid=126, pw_gid=135, pw_gecos='Jenkins,,,', pw_dir='/var/lib/jenkins', pw_shell='/bin/bash')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getpwnam('jenkins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T03:56:21.044019Z",
     "start_time": "2019-06-17T03:56:21.030487Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(os.getegid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T04:18:02.764454Z",
     "start_time": "2019-06-17T04:18:02.749660Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shutil.chown(dpath + 'raw_pmt_hist_1.fth', user='justin', group='justin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T04:17:42.069404Z",
     "start_time": "2019-06-17T04:09:10.002674Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/justin/anaconda3/lib/python3.6/shutil.py\u001b[0m(1046)\u001b[0;36mchown\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1044 \u001b[0;31m            \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no such group: {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1045 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1046 \u001b[0;31m    \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1047 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1048 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mget_terminal_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> _user\n",
      "1000\n",
      "ipdb> _group\n",
      "135\n",
      "ipdb> os.chown(path, _user, _group)\n",
      "*** PermissionError: [Errno 1] Operation not permitted: '/home/justin/projects/lendingclub/data/raw_pmt_hist_1.fth'\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T04:08:54.245340Z",
     "start_time": "2019-06-17T04:08:54.221149Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: '/home/justin/projects/lendingclub/data/raw_loan_info.fth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-30126ae235c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'raw_loan_info.fth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jenkins'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mchown\u001b[0;34m(path, user, group)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no such group: {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_terminal_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: '/home/justin/projects/lendingclub/data/raw_loan_info.fth'"
     ]
    }
   ],
   "source": [
    "shutil.chown(dpath + 'raw_loan_info.fth', group='jenkins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T03:31:51.355672Z",
     "start_time": "2019-06-17T03:31:51.327829Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: '/home/justin/projects/lendingclub/data/raw_pmt_hist_1.fth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-f84e3da6f4c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'raw_pmt_hist_1.fth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m126\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m135\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: '/home/justin/projects/lendingclub/data/raw_pmt_hist_1.fth'"
     ]
    }
   ],
   "source": [
    "os.chown(dpath + 'raw_pmt_hist_1.fth', 126, 135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:44:21.867832Z",
     "start_time": "2019-06-17T02:44:21.845342Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: '/home/justin/projects/lendingclub/data/raw_loan_info.fth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-4f4c6814be56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'raw_loan_info.fth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0o664\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: '/home/justin/projects/lendingclub/data/raw_loan_info.fth'"
     ]
    }
   ],
   "source": [
    "os.chmod(dpath + 'raw_loan_info.fth', 0o664)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:43:43.227144Z",
     "start_time": "2019-06-17T02:43:43.208854Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.chmod(dpath + 'raw_pmt_hist_1.fth', 0o664)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T00:10:28.989821Z",
     "start_time": "2019-06-16T00:10:28.740965Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mDockerfile\u001b[0m*                \u001b[01;32mJenkinsfile\u001b[0m*     README.md\r\n",
      "\u001b[01;32meval_results.py\u001b[0m*           \u001b[01;32mmodels.py\u001b[0m*       \u001b[01;32mtest_munging.py\u001b[0m*\r\n",
      "\u001b[01;32mfeature_pruning.py\u001b[0m*        \u001b[01;32mmunging.py\u001b[0m*      writing_the_modules.ipynb\r\n",
      "\u001b[01;32mhypothesis_example.ipynb\u001b[0m*  \u001b[01;32mother_utils.py\u001b[0m*\r\n",
      "\u001b[01;32m__init__.py\u001b[0m*               \u001b[34;42m__pycache__\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T02:40:47.170232Z",
     "start_time": "2019-06-17T02:40:47.137444Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.chown??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:46:54.530035Z",
     "start_time": "2019-07-25T01:46:54.514293Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:47:01.577339Z",
     "start_time": "2019-07-25T01:46:56.965161Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_feather(os.path.join(os.path.expanduser('~'), 'projects', 'lendingclub', 'data', 'raw_loan_info.fth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:47:17.115858Z",
     "start_time": "2019-07-25T01:47:17.083343Z"
    }
   },
   "outputs": [],
   "source": [
    "from munging import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-25T01:48:39.607755Z",
     "start_time": "2019-07-25T01:47:20.463215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to change columns to smaller dtypes when possible\n",
      "original dataframe is 5266.260987281799 MB or 5.142832995392382 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed dtypes of 1 cols\n",
      "reduced dataframe is 5257.195958137512 MB or 5.133980427868664 GB\n"
     ]
    }
   ],
   "source": [
    "changed_type_cols, df = reduce_memory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T02:45:33.498886Z",
     "start_time": "2019-06-14T02:45:32.276427Z"
    }
   },
   "outputs": [],
   "source": [
    "ori_df = df.copy()\n",
    "ori_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:44:20.605302Z",
     "start_time": "2019-06-14T15:44:19.396726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2376343, 149)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ori_df.copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:48:29.081227Z",
     "start_time": "2019-06-14T15:48:28.538867Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes([np.number]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T16:01:01.844204Z",
     "start_time": "2019-06-14T16:01:01.824138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate',\n",
       "       'installment', 'annual_inc', 'dti', 'delinq_2yrs',\n",
       "       ...\n",
       "       'deferral_term', 'hardship_amount', 'hardship_length', 'hardship_dpd',\n",
       "       'orig_projected_additional_accrued_interest',\n",
       "       'hardship_payoff_balance_amount', 'hardship_last_payment_amount',\n",
       "       'settlement_amount', 'settlement_percentage', 'settlement_term'],\n",
       "      dtype='object', length=113)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T16:03:08.739954Z",
     "start_time": "2019-06-14T16:03:08.527228Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'recoveries': 151435.99853515625, 'hardship_amount': 1139.222240447998},\n",
       " {'recoveries': 0.0, 'hardship_amount': 2.692669630050659e-05})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_noninf_val(df[['recoveries', 'hardship_amount']].div(df['delinq_amnt'], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T02:51:20.687164Z",
     "start_time": "2019-06-14T02:49:08.777845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping the following cols: \n",
      "['hardship_type', 'policy_code', 'deferral_term', 'hardship_length', 'policy_code_isnull', 'deferral_term_isnull', 'hardship_length_isnull']\n",
      "only 2 values, consider dropping the following cols: \n",
      "['pymnt_plan', 'initial_list_status', 'application_type', 'hardship_flag', 'debt_settlement_flag', 'term']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>pymnt_plan</th>\n",
       "      <th>url</th>\n",
       "      <th>...</th>\n",
       "      <th>sec_app_collections_12_mths_ex_med_isnull</th>\n",
       "      <th>sec_app_mths_since_last_major_derog_isnull</th>\n",
       "      <th>hardship_amount_isnull</th>\n",
       "      <th>hardship_dpd_isnull</th>\n",
       "      <th>orig_projected_additional_accrued_interest_isnull</th>\n",
       "      <th>hardship_payoff_balance_amount_isnull</th>\n",
       "      <th>hardship_last_payment_amount_isnull</th>\n",
       "      <th>settlement_amount_isnull</th>\n",
       "      <th>settlement_percentage_isnull</th>\n",
       "      <th>settlement_term_isnull</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>2.376343e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.644243e+00</td>\n",
       "      <td>1.118249e+01</td>\n",
       "      <td>2.798029e+05</td>\n",
       "      <td>4.415229e+00</td>\n",
       "      <td>3.914463e+00</td>\n",
       "      <td>1.936336e+00</td>\n",
       "      <td>7.494740e+01</td>\n",
       "      <td>3.894764e+00</td>\n",
       "      <td>1.000205e+00</td>\n",
       "      <td>1.188172e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>9.475236e-01</td>\n",
       "      <td>9.828127e-01</td>\n",
       "      <td>9.951640e-01</td>\n",
       "      <td>9.951640e-01</td>\n",
       "      <td>9.961862e-01</td>\n",
       "      <td>9.951640e-01</td>\n",
       "      <td>9.951640e-01</td>\n",
       "      <td>9.843171e-01</td>\n",
       "      <td>9.843171e-01</td>\n",
       "      <td>9.843171e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.255025e+00</td>\n",
       "      <td>6.283129e+00</td>\n",
       "      <td>1.481650e+05</td>\n",
       "      <td>3.151404e+00</td>\n",
       "      <td>1.913116e+00</td>\n",
       "      <td>7.778774e-01</td>\n",
       "      <td>4.052204e+01</td>\n",
       "      <td>2.191074e+00</td>\n",
       "      <td>1.431415e-02</td>\n",
       "      <td>6.859913e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.229858e-01</td>\n",
       "      <td>1.299690e-01</td>\n",
       "      <td>6.937303e-02</td>\n",
       "      <td>6.937303e-02</td>\n",
       "      <td>6.163846e-02</td>\n",
       "      <td>6.937303e-02</td>\n",
       "      <td>6.937303e-02</td>\n",
       "      <td>1.242456e-01</td>\n",
       "      <td>1.242456e-01</td>\n",
       "      <td>1.242456e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>1.507050e+05</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.940865e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>2.982590e+05</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>7.700000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.188172e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>4.181740e+05</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.130000e+02</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.782258e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>3.500000e+01</td>\n",
       "      <td>5.268560e+05</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.420000e+02</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.376343e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              grade     sub_grade     emp_title    emp_length  home_ownership  \\\n",
       "count  2.376343e+06  2.376343e+06  2.376343e+06  2.376343e+06    2.376343e+06   \n",
       "mean   2.644243e+00  1.118249e+01  2.798029e+05  4.415229e+00    3.914463e+00   \n",
       "std    1.255025e+00  6.283129e+00  1.481650e+05  3.151404e+00    1.913116e+00   \n",
       "min    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00    1.000000e+00   \n",
       "25%    2.000000e+00  6.000000e+00  1.507050e+05  2.000000e+00    2.000000e+00   \n",
       "50%    3.000000e+00  1.100000e+01  2.982590e+05  3.000000e+00    5.000000e+00   \n",
       "75%    3.000000e+00  1.500000e+01  4.181740e+05  6.000000e+00    6.000000e+00   \n",
       "max    7.000000e+00  3.500000e+01  5.268560e+05  1.100000e+01    6.000000e+00   \n",
       "\n",
       "       verification_status       issue_d   loan_status    pymnt_plan  \\\n",
       "count         2.376343e+06  2.376343e+06  2.376343e+06  2.376343e+06   \n",
       "mean          1.936336e+00  7.494740e+01  3.894764e+00  1.000205e+00   \n",
       "std           7.778774e-01  4.052204e+01  2.191074e+00  1.431415e-02   \n",
       "min           1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "25%           1.000000e+00  4.200000e+01  2.000000e+00  1.000000e+00   \n",
       "50%           2.000000e+00  7.700000e+01  2.000000e+00  1.000000e+00   \n",
       "75%           3.000000e+00  1.130000e+02  6.000000e+00  1.000000e+00   \n",
       "max           3.000000e+00  1.420000e+02  9.000000e+00  2.000000e+00   \n",
       "\n",
       "                url  ...  sec_app_collections_12_mths_ex_med_isnull  \\\n",
       "count  2.376343e+06  ...                               2.376343e+06   \n",
       "mean   1.188172e+06  ...                               9.475236e-01   \n",
       "std    6.859913e+05  ...                               2.229858e-01   \n",
       "min    1.000000e+00  ...                               0.000000e+00   \n",
       "25%    5.940865e+05  ...                               1.000000e+00   \n",
       "50%    1.188172e+06  ...                               1.000000e+00   \n",
       "75%    1.782258e+06  ...                               1.000000e+00   \n",
       "max    2.376343e+06  ...                               1.000000e+00   \n",
       "\n",
       "       sec_app_mths_since_last_major_derog_isnull  hardship_amount_isnull  \\\n",
       "count                                2.376343e+06            2.376343e+06   \n",
       "mean                                 9.828127e-01            9.951640e-01   \n",
       "std                                  1.299690e-01            6.937303e-02   \n",
       "min                                  0.000000e+00            0.000000e+00   \n",
       "25%                                  1.000000e+00            1.000000e+00   \n",
       "50%                                  1.000000e+00            1.000000e+00   \n",
       "75%                                  1.000000e+00            1.000000e+00   \n",
       "max                                  1.000000e+00            1.000000e+00   \n",
       "\n",
       "       hardship_dpd_isnull  orig_projected_additional_accrued_interest_isnull  \\\n",
       "count         2.376343e+06                                       2.376343e+06   \n",
       "mean          9.951640e-01                                       9.961862e-01   \n",
       "std           6.937303e-02                                       6.163846e-02   \n",
       "min           0.000000e+00                                       0.000000e+00   \n",
       "25%           1.000000e+00                                       1.000000e+00   \n",
       "50%           1.000000e+00                                       1.000000e+00   \n",
       "75%           1.000000e+00                                       1.000000e+00   \n",
       "max           1.000000e+00                                       1.000000e+00   \n",
       "\n",
       "       hardship_payoff_balance_amount_isnull  \\\n",
       "count                           2.376343e+06   \n",
       "mean                            9.951640e-01   \n",
       "std                             6.937303e-02   \n",
       "min                             0.000000e+00   \n",
       "25%                             1.000000e+00   \n",
       "50%                             1.000000e+00   \n",
       "75%                             1.000000e+00   \n",
       "max                             1.000000e+00   \n",
       "\n",
       "       hardship_last_payment_amount_isnull  settlement_amount_isnull  \\\n",
       "count                         2.376343e+06              2.376343e+06   \n",
       "mean                          9.951640e-01              9.843171e-01   \n",
       "std                           6.937303e-02              1.242456e-01   \n",
       "min                           0.000000e+00              0.000000e+00   \n",
       "25%                           1.000000e+00              1.000000e+00   \n",
       "50%                           1.000000e+00              1.000000e+00   \n",
       "75%                           1.000000e+00              1.000000e+00   \n",
       "max                           1.000000e+00              1.000000e+00   \n",
       "\n",
       "       settlement_percentage_isnull  settlement_term_isnull  \n",
       "count                  2.376343e+06            2.376343e+06  \n",
       "mean                   9.843171e-01            9.843171e-01  \n",
       "std                    1.242456e-01            1.242456e-01  \n",
       "min                    0.000000e+00            0.000000e+00  \n",
       "25%                    1.000000e+00            1.000000e+00  \n",
       "50%                    1.000000e+00            1.000000e+00  \n",
       "75%                    1.000000e+00            1.000000e+00  \n",
       "max                    1.000000e+00            1.000000e+00  \n",
       "\n",
       "[8 rows x 255 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, max_dict, min_dict, new_null_colnames, fill_dict, cats_dict, norm_dict = train_proc(df)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T17:17:11.177701Z",
     "start_time": "2019-06-14T17:17:11.155758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'means': {'id': 83586239.45349346,\n",
       "  'loan_amnt': 15156.7783203125,\n",
       "  'funded_amnt': 15151.6904296875,\n",
       "  'funded_amnt_inv': 15134.6689453125,\n",
       "  'term': 42.95010779167822,\n",
       "  'int_rate': 13.033180236816406,\n",
       "  'installment': 447.61492919921875,\n",
       "  'annual_inc': 78292.375,\n",
       "  'dti': 18.901901245117188,\n",
       "  'delinq_2yrs': 0.3030197322368622,\n",
       "  'fico_range_low': 694.5877685546875,\n",
       "  'fico_range_high': 699.08203125,\n",
       "  'inq_last_6mths': 0.5703238844871521,\n",
       "  'mths_since_last_delinq': 34.630218505859375,\n",
       "  'mths_since_last_record': 72.87171173095703,\n",
       "  'open_acc': 11.617355346679688,\n",
       "  'pub_rec': 0.1936313956975937,\n",
       "  'revol_bal': 16713.69140625,\n",
       "  'total_acc': 24.111675262451172,\n",
       "  'out_prncp': 4289.98828125,\n",
       "  'out_prncp_inv': 4289.1025390625,\n",
       "  'total_pymnt': 12078.12109375,\n",
       "  'total_pymnt_inv': 12060.4921875,\n",
       "  'total_rec_prncp': 9518.5087890625,\n",
       "  'total_rec_int': 2421.72998046875,\n",
       "  'total_rec_late_fee': 1.5540074110031128,\n",
       "  'recoveries': 148.2235870361328,\n",
       "  'collection_recovery_fee': 24.83300018310547,\n",
       "  'last_pymnt_amnt': 3424.431396484375,\n",
       "  'last_fico_range_high': 688.4393310546875,\n",
       "  'last_fico_range_low': 675.9913940429688,\n",
       "  'collections_12_mths_ex_med': 0.018075093626976013,\n",
       "  'mths_since_last_major_derog': 44.25545883178711,\n",
       "  'policy_code': 1.0,\n",
       "  'annual_inc_joint': 125525.7421875,\n",
       "  'dti_joint': 19.286821365356445,\n",
       "  'acc_now_delinq': 0.003946027252823114,\n",
       "  'tot_coll_amt': 229.9242401123047,\n",
       "  'tot_cur_bal': 142972.875,\n",
       "  'open_acc_6m': 0.9328386187553406,\n",
       "  'open_act_il': 2.7802510261535645,\n",
       "  'open_il_12m': 0.6783244013786316,\n",
       "  'open_il_24m': 1.5645875930786133,\n",
       "  'mths_since_rcnt_il': 21.139366149902344,\n",
       "  'total_bal_il': 35665.22265625,\n",
       "  'il_util': 69.04531860351562,\n",
       "  'open_rv_12m': 1.2807635068893433,\n",
       "  'open_rv_24m': 2.727975606918335,\n",
       "  'max_bal_bc': 5853.044921875,\n",
       "  'all_util': 56.81615447998047,\n",
       "  'total_rev_hi_lim': 34919.265625,\n",
       "  'inq_fi': 1.0210049152374268,\n",
       "  'total_cu_tl': 1.4825438261032104,\n",
       "  'inq_last_12m': 2.0245137214660645,\n",
       "  'acc_open_past_24mths': 4.511463165283203,\n",
       "  'avg_cur_bal': 13585.568359375,\n",
       "  'bc_open_to_buy': 11620.5322265625,\n",
       "  'bc_util': 57.468048095703125,\n",
       "  'chargeoff_within_12_mths': 0.008415965363383293,\n",
       "  'delinq_amnt': 11.915594100952148,\n",
       "  'mo_sin_old_il_acct': 125.7259750366211,\n",
       "  'mo_sin_old_rev_tl_op': 181.25906372070312,\n",
       "  'mo_sin_rcnt_rev_tl_op': 14.100574493408203,\n",
       "  'mo_sin_rcnt_tl': 8.31757640838623,\n",
       "  'mort_acc': 1.5465898513793945,\n",
       "  'mths_since_recent_bc': 24.9018611907959,\n",
       "  'mths_since_recent_bc_dlq': 39.31877517700195,\n",
       "  'mths_since_recent_inq': 7.055539131164551,\n",
       "  'mths_since_recent_revol_delinq': 35.845951080322266,\n",
       "  'num_accts_ever_120_pd': 0.49700725078582764,\n",
       "  'num_actv_bc_tl': 3.6793744564056396,\n",
       "  'num_actv_rev_tl': 5.625213623046875,\n",
       "  'num_bc_sats': 4.786467552185059,\n",
       "  'num_bc_tl': 7.700404167175293,\n",
       "  'num_il_tl': 8.415186882019043,\n",
       "  'num_op_rev_tl': 8.249537467956543,\n",
       "  'num_rev_accts': 13.958802223205566,\n",
       "  'num_rev_tl_bal_gt_0': 5.574110507965088,\n",
       "  'num_sats': 11.631584167480469,\n",
       "  'num_tl_120dpd_2m': 0.0006050803349353373,\n",
       "  'num_tl_30dpd': 0.0026725155767053366,\n",
       "  'num_tl_90g_dpd_24m': 0.08168713003396988,\n",
       "  'num_tl_op_past_12m': 2.072521209716797,\n",
       "  'pct_tl_nvr_dlq': 93.1662826538086,\n",
       "  'percent_bc_gt_75': 41.67689514160156,\n",
       "  'pub_rec_bankruptcies': 0.1276698112487793,\n",
       "  'tax_liens': 0.044494278728961945,\n",
       "  'tot_hi_cred_lim': 179188.046875,\n",
       "  'total_bal_ex_mort': 51259.90625,\n",
       "  'total_bc_limit': 23476.28515625,\n",
       "  'total_il_high_credit_limit': 43970.6171875,\n",
       "  'revol_bal_joint': 34382.4140625,\n",
       "  'sec_app_fico_range_low': 670.9031372070312,\n",
       "  'sec_app_fico_range_high': 674.9142456054688,\n",
       "  'sec_app_inq_last_6mths': 0.6240637898445129,\n",
       "  'sec_app_mort_acc': 1.5535436868667603,\n",
       "  'sec_app_open_acc': 11.501355171203613,\n",
       "  'sec_app_revol_util': 57.87659454345703,\n",
       "  'sec_app_open_act_il': 3.0202643871307373,\n",
       "  'sec_app_num_rev_accts': 12.550215721130371,\n",
       "  'sec_app_chargeoff_within_12_mths': 0.04380042105913162,\n",
       "  'sec_app_collections_12_mths_ex_med': 0.07421693205833435,\n",
       "  'sec_app_mths_since_last_major_derog': 37.214576721191406,\n",
       "  'deferral_term': 3.0,\n",
       "  'hardship_amount': 155.24905395507812,\n",
       "  'hardship_length': 3.0,\n",
       "  'hardship_dpd': 13.846675872802734,\n",
       "  'orig_projected_additional_accrued_interest': 455.022705078125,\n",
       "  'hardship_payoff_balance_amount': 11670.8583984375,\n",
       "  'hardship_last_payment_amount': 194.51084899902344,\n",
       "  'settlement_amount': 4992.45263671875,\n",
       "  'settlement_percentage': 47.75641632080078,\n",
       "  'settlement_term': 13.429027557373047},\n",
       " 'stds': {'id': 46189830.09452807,\n",
       "  'loan_amnt': 9253.728515625,\n",
       "  'funded_amnt': 9252.0244140625,\n",
       "  'funded_amnt_inv': 9263.2666015625,\n",
       "  'term': 10.885707994903072,\n",
       "  'int_rate': 4.836043357849121,\n",
       "  'installment': 268.2271423339844,\n",
       "  'annual_inc': 112291.140625,\n",
       "  'dti': 14.536628723144531,\n",
       "  'delinq_2yrs': 0.864122748374939,\n",
       "  'fico_range_low': 33.39722442626953,\n",
       "  'fico_range_high': 33.43531036376953,\n",
       "  'inq_last_6mths': 0.8809492588043213,\n",
       "  'mths_since_last_delinq': 21.899911880493164,\n",
       "  'mths_since_last_record': 26.465394973754883,\n",
       "  'open_acc': 5.655346393585205,\n",
       "  'pub_rec': 0.5629160404205322,\n",
       "  'revol_bal': 22908.9765625,\n",
       "  'total_acc': 11.974811553955078,\n",
       "  'out_prncp': 7440.3525390625,\n",
       "  'out_prncp_inv': 7439.9365234375,\n",
       "  'total_pymnt': 9970.80078125,\n",
       "  'total_pymnt_inv': 9966.443359375,\n",
       "  'total_rec_prncp': 8371.0732421875,\n",
       "  'total_rec_int': 2687.763916015625,\n",
       "  'total_rec_late_fee': 12.139945983886719,\n",
       "  'recoveries': 758.176513671875,\n",
       "  'collection_recovery_fee': 133.66168212890625,\n",
       "  'last_pymnt_amnt': 6008.173828125,\n",
       "  'last_fico_range_high': 73.28876495361328,\n",
       "  'last_fico_range_low': 111.75546264648438,\n",
       "  'collections_12_mths_ex_med': 0.14949333667755127,\n",
       "  'mths_since_last_major_derog': 21.549612045288086,\n",
       "  'policy_code': 0.0,\n",
       "  'annual_inc_joint': 75104.234375,\n",
       "  'dti_joint': 7.856663227081299,\n",
       "  'acc_now_delinq': 0.06781544536352158,\n",
       "  'tot_coll_amt': 8306.96484375,\n",
       "  'tot_cur_bal': 161163.546875,\n",
       "  'open_acc_6m': 1.1387271881103516,\n",
       "  'open_act_il': 2.9905757904052734,\n",
       "  'open_il_12m': 0.9278503060340881,\n",
       "  'open_il_24m': 1.5796773433685303,\n",
       "  'mths_since_rcnt_il': 25.943981170654297,\n",
       "  'total_bal_il': 44375.46484375,\n",
       "  'il_util': 23.722402572631836,\n",
       "  'open_rv_12m': 1.504433274269104,\n",
       "  'open_rv_24m': 2.5868148803710938,\n",
       "  'max_bal_bc': 5706.50244140625,\n",
       "  'all_util': 20.8670711517334,\n",
       "  'total_rev_hi_lim': 36770.06640625,\n",
       "  'inq_fi': 1.4859628677368164,\n",
       "  'total_cu_tl': 2.6620898246765137,\n",
       "  'inq_last_12m': 2.374595880508423,\n",
       "  'acc_open_past_24mths': 3.1399946212768555,\n",
       "  'avg_cur_bal': 16525.4453125,\n",
       "  'bc_open_to_buy': 16829.380859375,\n",
       "  'bc_util': 28.620210647583008,\n",
       "  'chargeoff_within_12_mths': 0.10407395660877228,\n",
       "  'delinq_amnt': 712.1317138671875,\n",
       "  'mo_sin_old_il_acct': 53.47123336791992,\n",
       "  'mo_sin_old_rev_tl_op': 97.21513366699219,\n",
       "  'mo_sin_rcnt_rev_tl_op': 17.63108253479004,\n",
       "  'mo_sin_rcnt_tl': 9.192151069641113,\n",
       "  'mort_acc': 1.903542399406433,\n",
       "  'mths_since_recent_bc': 32.382415771484375,\n",
       "  'mths_since_recent_bc_dlq': 22.60090446472168,\n",
       "  'mths_since_recent_inq': 5.95184850692749,\n",
       "  'mths_since_recent_revol_delinq': 22.301372528076172,\n",
       "  'num_accts_ever_120_pd': 1.349995732307434,\n",
       "  'num_actv_bc_tl': 2.3257577419281006,\n",
       "  'num_actv_rev_tl': 3.382432222366333,\n",
       "  'num_bc_sats': 3.0499141216278076,\n",
       "  'num_bc_tl': 4.6908278465271,\n",
       "  'num_il_tl': 7.351287841796875,\n",
       "  'num_op_rev_tl': 4.71735954284668,\n",
       "  'num_rev_accts': 8.013647079467773,\n",
       "  'num_rev_tl_bal_gt_0': 3.2861878871917725,\n",
       "  'num_sats': 5.656951904296875,\n",
       "  'num_tl_120dpd_2m': 0.026404011994600296,\n",
       "  'num_tl_30dpd': 0.05468745902180672,\n",
       "  'num_tl_90g_dpd_24m': 0.4857105612754822,\n",
       "  'num_tl_op_past_12m': 1.8319387435913086,\n",
       "  'pct_tl_nvr_dlq': 9.096850395202637,\n",
       "  'percent_bc_gt_75': 36.25564193725586,\n",
       "  'pub_rec_bankruptcies': 0.36474472284317017,\n",
       "  'tax_liens': 0.36654797196388245,\n",
       "  'tot_hi_cred_lim': 182141.1875,\n",
       "  'total_bal_ex_mort': 50179.3046875,\n",
       "  'total_bc_limit': 23232.455078125,\n",
       "  'total_il_high_credit_limit': 45295.796875,\n",
       "  'revol_bal_joint': 28934.8359375,\n",
       "  'sec_app_fico_range_low': 44.610107421875,\n",
       "  'sec_app_fico_range_high': 44.60670471191406,\n",
       "  'sec_app_inq_last_6mths': 0.9833303093910217,\n",
       "  'sec_app_mort_acc': 1.7653918266296387,\n",
       "  'sec_app_open_acc': 6.634374141693115,\n",
       "  'sec_app_revol_util': 25.563283920288086,\n",
       "  'sec_app_open_act_il': 3.2692461013793945,\n",
       "  'sec_app_num_rev_accts': 8.161091804504395,\n",
       "  'sec_app_chargeoff_within_12_mths': 0.39475589990615845,\n",
       "  'sec_app_collections_12_mths_ex_med': 0.39664849638938904,\n",
       "  'sec_app_mths_since_last_major_derog': 23.907129287719727,\n",
       "  'deferral_term': 0.0,\n",
       "  'hardship_amount': 129.15716552734375,\n",
       "  'hardship_length': 0.0,\n",
       "  'hardship_dpd': 9.588881492614746,\n",
       "  'orig_projected_additional_accrued_interest': 375.2062072753906,\n",
       "  'hardship_payoff_balance_amount': 7659.181640625,\n",
       "  'hardship_last_payment_amount': 197.97117614746094,\n",
       "  'settlement_amount': 3690.740234375,\n",
       "  'settlement_percentage': 7.21049165725708,\n",
       "  'settlement_term': 8.106500625610352},\n",
       " 'std_notfin': {}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:41:29.190129Z",
     "start_time": "2019-06-14T15:41:29.170728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T02:13:20.359901Z",
     "start_time": "2019-06-14T02:11:50.790322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping the following cols: \n",
      "['member_id', 'policy_code', 'hardship_type', 'deferral_term', 'hardship_length', 'member_id_isnull', 'policy_code_isnull', 'deferral_term_isnull', 'hardship_length_isnull']\n",
      "only 2 values, consider dropping the following cols: \n",
      "['term', 'pymnt_plan', 'initial_list_status', 'application_type', 'hardship_flag', 'debt_settlement_flag']\n"
     ]
    }
   ],
   "source": [
    "proced_train = train_proc(ex_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T02:16:12.443136Z",
     "start_time": "2019-06-14T02:16:12.424013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # old\n",
    "    \n",
    "# # def get_medians_make_nullcols_fill_values(df, na_map = {}, catboost=False):\n",
    "#     '''\n",
    "#     This function makes new columns marking if a column is null or not, and\n",
    "#     then fills the original column with median values. Does this inplace on\n",
    "#     the passed dataframe, but still returns the dataframe as well. IF col \n",
    "#     dtype is not numeric, fills with mode instead. IF df[col].median() is \n",
    "#     nan, fills with 0\n",
    "#     '''\n",
    "#     if not na_map:\n",
    "#         has_nulls = [col for col in df.columns if any(df[col].isnull())]\n",
    "#         for col in tqdm(has_nulls):\n",
    "#             if catboost:\n",
    "#                 # catboost wanted nans in string dtypes to be string...\n",
    "#                 if is_string_dtype(df[col]):\n",
    "#                     df[col] = df[col].replace(np.nan, 'nan')\n",
    "#                     na_map[col] = 'nan'\n",
    "\n",
    "#             # normal stuff\n",
    "#             if is_numeric_dtype(df[col]):\n",
    "#                 if np.isnan(df[col].median()):\n",
    "#                     # handles case where the whole col is nan and you don't want to drop\n",
    "#                     # for reasons (old data doesn't have feature, but new data does)\n",
    "#                     df[col] = df[col].fillna(0)\n",
    "#                     na_map[col] = 0\n",
    "#                 else:\n",
    "#                     median = df[col].median()\n",
    "#                     df[col] = df[col].fillna(median)\n",
    "#                     na_map[col] = median\n",
    "#             else:\n",
    "#                 print('{0} col is likely of non-int/float dtype, filling with mode instead'.format(col))\n",
    "#                 mode = df[col].mode() #is pandas series\n",
    "#                 if len(mode) == 0:\n",
    "#                     mode = 0\n",
    "#                 else:\n",
    "#                     mode = np.random.choice(mode) #if len(1), return mode, else ranodmly choose 1\n",
    "#                 df[col] = df[col].fillna(mode)\n",
    "#                 na_map[col] = mode\n",
    "#         return df, na_map\n",
    "#     else:\n",
    "#         print('Was passed an na_map. Will fill accordingly (test or valid data?)')\n",
    "#         for col, nafill in na_map.items():\n",
    "#             df[col] = df[col].fillna(nafill)\n",
    "#         return df\n",
    "\n",
    "# # def prepare_nulls_valid_test(valid_test, train, \n",
    "\n",
    "# # def normalize_df(df, means_stds = {}):\n",
    "# #     '''\n",
    "# #     This function should be run after get_medians_make_nullcols_fill_values.\n",
    "# #     Doesn't normalize the \"_isnull\" cols that are added by the aforemetioned\n",
    "# #     function. Will print out a list of columns that it did not normalize but\n",
    "# #     those cols should be checked if normalization is actually desired. \n",
    "# #     means_stds are for valid or test sets, normalized with same values as train\n",
    "# #     '''\n",
    "# #     if not means_stds:\n",
    "# #         unsure_cols = []\n",
    "# #         means = {}\n",
    "# #         stds = {}\n",
    "# #         for col in tqdm(df.columns):\n",
    "# #             if '_isnull' not in col:\n",
    "# #                 mean = df[col].mean()\n",
    "# #                 means[col] = mean\n",
    "# #                 std = df[col].std()\n",
    "# #                 stds[col] = std\n",
    "# #                 df[col] = (df[col]-mean)/std\n",
    "# #             elif '_isnull' in col:\n",
    "# #                 pass\n",
    "# #             else:\n",
    "# #                 if 'null' in col.lower() or 'is_null' in col.lower():\n",
    "# #                     unsure_cols.append(col)\n",
    "# #         print('These cols have word null or is_null in them. Double check. {0}'.format(\n",
    "# #             unsure_cols))\n",
    "# #         means_stds['means'] = means\n",
    "# #         means_stds['stds'] = stds\n",
    "# #         return df, means_stds\n",
    "# #     else:\n",
    "# #         print('Passed means_stds, normalizing cols according to means_stds (is valid or test set)')\n",
    "# #         means = means_stds['means']\n",
    "# #         stds = means_stds['stds']\n",
    "# #         for col in means.keys():\n",
    "# #             df[col] = (df[col] - means[col])/stds[col]\n",
    "# #         return df\n",
    "    \n",
    "# # def jproc_df(df, target=None, one_hot=False, copy=True, summary=True): #broken out into functions below, probably deprecated?\n",
    "#     '''\n",
    "#     Should be run on df after:\n",
    "#     1. mg.transform_dates, (turns datetime columns into ML usable)\n",
    "#     2. mg.remove_zerovar_cols, and consider drop cols is examined\n",
    "#     3. fastai's train_cats (turns obj/str cols into categorical)\n",
    "    \n",
    "#     This function will convert categoricals to their codes, adding +1 (so nan\n",
    "#     is 0 instead of -1), and then will calculate means/stddev for\n",
    "#     normalizing and median for filling nan values. Also creates new cols\n",
    "#     demarkating whether value was originally nan via\n",
    "#     mg.get_medians_make_nullcols_fill_values.\n",
    "    \n",
    "#     copy could be set to False if you suspect memory issues\n",
    "    \n",
    "#     returns x, y, na_dict, mapper\n",
    "#     '''\n",
    "    \n",
    "#     if target:\n",
    "#         y = df[target]\n",
    "#     else:\n",
    "#         print('No specified target column, assuming target already separated')\n",
    "#         y = []\n",
    "#     if copy:\n",
    "#         df = df.copy()\n",
    "    \n",
    "#     # deal with cat cols, can either onehot or just turn into the categorical code\n",
    "#     cat_cols = df.select_dtypes('category').columns\n",
    "#     if one_hot:\n",
    "#         print('Turning categoricals into one_hot representation')\n",
    "#         dummied = pd.get_dummies(df[cat_cols])\n",
    "#         df.drop(cat_cols, axis=1, inplace=True)\n",
    "#     else:\n",
    "#         print('Converting categoricals to their codes . . .')\n",
    "#         for col in tqdm(cat_cols):\n",
    "#             df[col] = df[col].cat.codes+1\n",
    "    \n",
    "#     # all other (numeric) cols\n",
    "#     # gather the means/stddevs/nas __________________\n",
    "#     print('Calculating means/medians/std_devs . . .')\n",
    "#     all_other_cols = [col for col in df.columns if col not in cat_cols]\n",
    "#     mapper = {}\n",
    "#     na_dict = {}\n",
    "#     for col in tqdm(all_other_cols):\n",
    "#         mapper[col] = {'mean': df[col].mean(),\n",
    "#                        'std_dev': df[col].std()}\n",
    "#         na_dict[col] = df[col].median()\n",
    "        \n",
    "#     # make na cols, fill nas with median\n",
    "#     print('Making _isnull indicator columns . . .')\n",
    "#     has_nulls = [col for col in df.columns if any(df[col].isnull())]\n",
    "#     for col in tqdm(has_nulls):\n",
    "#         df[col+'_isnull'] = np.where(df[col].isnull(), 1, 0)\n",
    "#         df[col] = df[col].fillna(na_dict[col])\n",
    "        \n",
    "#     # normalize the df excluding cat_cols\n",
    "#     print('Normalizing all non-categorical and non-_isnull columns . . .')\n",
    "#     all_other_cols = [col for col in df.columns if col not in cat_cols]\n",
    "#     for col in tqdm(all_other_cols):\n",
    "#         if '_isnull' not in col:\n",
    "#             df[col] = (df[col]-mapper[col]['mean'])/mapper[col]['std_dev']\n",
    "    \n",
    "#     if one_hot:\n",
    "#         df = pd.concat([df, dummied], axis=1)\n",
    "    \n",
    "#     if summary:\n",
    "#         print('Categorical cols: {0}\\n\\n'.format(list(cat_cols)))\n",
    "#         print('Made _isnull cols for: {0}\\n\\n'.format(list(has_nulls)))\n",
    "#         print('Normalized all other cols: {0}\\n\\n'.format([col for col in all_other_cols if '_isnull' not in col]))\n",
    "            \n",
    "#     return df, y, na_dict, mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
